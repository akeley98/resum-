I'm currently working as an IC3 software engineer at Nvidia, and have several years of experience independently designing, hand-optimizing, and (inevitably) debugging parallel algorithms motivated by real-world need. In my role as a developer technology engineer, I work both with Nvidia Research and directly with customers to design new GPU algorithms and libraries.

My interests lie in designing disciplined abstractions that allow for non-experts to efficiently leverage parallel processors, or for experts to do so more safely and efficiently. Just as it is easy today for a research scientist to program a single x86 core (maybe with the help of a wealth of Python packages), so too may the same be said eventually for programming heterogeneous parallel processors. In other words, I'm looking to research abstractions that could one day help make specialists like me redundant.

In the meantime, I look forward to being able to dedicate more time as a PhD student to understanding the theory of parallel algorithms, programming languages, and compiler optimization at a deeper and more disciplined level than my day-to-day job allows for, and apply that knowledge, along with my existing industry experience working on substantial codebases, to contribute to parallel-programming frameworks, DSLs, or auto-scheduling research.

Aetherling:

I first contributed to this field while working at Stanford on the Aetherling FPGA DSL ("Type-Directed Scheduling of Streaming Accelerators", PLDI 2020). This Haskell-embedded DSL compiles an input DAG for an image processing pipeline into an FPGA design, which is auto-parallelized (to fit the chip) using a type system that encodes interface and scheduling information for each DAG vertex.

In this early phase of the project, I made a couple of practical contributions (a functional simulator and some test cases for generated hardware designs), but most of my work was in prototyping language design and the type system. Most notably, I identified that the type system rules interacted poorly with the line buffer's 2D interface, and would have made it impossible (except special cases) to express a valid line buffer with a parallelization factor other than one. I proposed a new way of representing line buffer state that fixed this limitation, and assisted David Durst (the lead author) in seeing this new line buffer realized in a hardware design language.

I also prototyped type systems for encoding scheduling information (latency, lane count, underutilization) and algorithms for automatic delay-matching.

Question:

Like most such DSLs, this abstraction is specialized for representing tensor/image processing. This raises one of my long-term research questions: to what extent can such novel languages (e.g. Halide, spatial, low-energy) be generalized to accelerate a much broader class of algorithms, like those with more free-form, data-dependent memory access? Much of my day-to-day experience in implementing parallel algorithms (an experience I envision one day looking radically different) has been on such problems.

MediocrePy:

I faced such challenges in community college while working with Dr. Zheng Cai (UC Santa Cruz Astrophysics) to fix performance problems in their number-crunching codebase. We targetted the "image combine" and "sigma-clipping" steps for acceleration, which I re-implemented as a multithreaded AVX library, MediocrePy, that implements this algorithm over 100 times faster than the pure Python code replaced (I am the sole author).

In this algorithm, a short stack of 2D images is combined into a single image, where the output pixel (x,y) is a function of the stack of (x,y) pixels taken from each input image.  This "combine" function iteratively takes the mean and standard deviation of unmasked pixel values, masks out outliers, and repeats an unpredictable number of times (independent per pixel stack) until convergence. Although nominally an image processing problem, with each pixel stack being its own independent lane, vectorizing this was
complicated by an unusual amount of data-dependent control flow and memory addressing.

NVIDIA:

My current 2D computational geometry research at Nvidia (co-presented with Mark Kilgard at GTC 2023, s51140) also yielded many such difficult-to-parallelize algorithms. One of them, fitting cubic Bézier segments to input point strips, is most naturally represented as a recursive sequential algorithm: we reduce the input points into a cubic segment, then, if the fit is poor, a heuristic chooses where to subdivide the input in two, and we recursively fit to both subdivided sequences. Note, the recursion is non-trivial, and the size of the reduction is data-dependent.

I transformed this to a GPU algorithm by:

1. Assigning one thread to each input point, and rewriting all loops.

2. Transforming the recursion to tail recursion (as each point/thread is now involved in just one branch of the subdivide recursion).

3. Efficiently parallelizing these unpredictable-length reductions, which pay no heed to warp or block boundaries.

Although I've taken pains to keep the codebase clear and documented, the fact remains the actual algorithm can only be seen through a distorting lens, dimly, past all the atomics, interleaved synchronization, and comments explaining these complicated transformations.  Wouldn't it be nice if there were clearer abstractions for expressing these manually-applied "rewrite rules"?

Future:

But of course, setting up a PL ambitious enough to abstract away messy problems like the above is probably somewhere between a "postdoc" and "never" project, and that's fine. My immediate interests are to contribute to, and gain experience from, more realistic ways to push the state of the art for parallel algorithms and accelerator design (in the spirit of past work like Halide, Exo, or Mosaic). As such, my interests align best with those of Dr. Saman Amarasinghe, Dr. Julian Shun, and Dr. Johnathan Ragan-Kelly. In the long term – maybe late PhD, maybe postdoc – and with the benefit of experience gained, I hope to be able to form a more informed opinion on what ways (and to what extent) such work can be extended to form a viable framework for accelerating general-purpose algorithms.
