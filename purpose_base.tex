\input{cv_style.tex}

\begin{document}
\raggedright
\reversemarginpar

\myKey{Introduction:}
I'm currently working as an IC3 software engineer at Nvidia, and have
several years of experience independently designing, hand-optimizing,
and (inevitably) debugging parallel algorithms motivated by real-world
need. In my role as a developer technology engineer, I work both with
Nvidia Research and directly with customers to design new GPU
algorithms and libraries.

My interests lie in designing disciplined abstractions that allow for
non-experts to efficiently leverage parallel processors, or for
experts to do so more safely and efficiently. Just as it is easy today
for a research scientist to program a single x86 core (maybe with the
help of a wealth of Python packages), so too may the same be said
eventually for programming heterogenous parallel processors. In other
words, I'm looking to research abstractions that could one day make
specialists like me redundant.

In the meantime, I look forward to being able to dedicate more time as
a PhD student to understanding the theory of parallel algorithms,
programming languages, and compiler optimization at a deeper and more
disciplined level than my day-to-day job allows for, and apply that
knowledge, along with my existing industry experience working on
substantial codebases, to contribute to parallel-programming
frameworks, DSLs, or auto-scheduling research. My past
hand-optimization experience has been on algorithms that
are \textit{not} really naturally represented as image, matrix, or
sparse tensor computation, so I'm particularly (but not exclusively)
curious about whether the work on parallel DSLs can be adapted to such
problems.

\myKey{MediocrePy:}
As a side project in community college, I worked
with \webText{https://astro.tsinghua.edu.cn/info/1039/1315.htm}{Dr. Zheng
Cai} (UC Santa Cruz Astrophysics) to understand the performance
problems of their number-crunching codebase. We targetted the ``image
combine'' and ``sigma-clipping'' steps for acceleration.

In this algorithm, a short stack of 2D images is combined into a
single image, where the output pixel $(x,y)$ is a function of the
stack of $(x,y)$ pixels taken from each input image.  The combine
function iteratively takes the mean/median and standard deviation of
unmasked pixel values, masks out outliers, and repeats an
unpredictable number of times (independent per pixel stack) until
convergence. Although nominally an image processing problem, with each
pixel stack being its own independent lane, this problem was
complicated by an unusual amount of data-dependent control flow and
memory addressing.

I am the sole author of a multithreaded AVX
library, \webText{https://github.com/akeley98/MediocrePy}{MediocrePy},
that implements this algorithm over $100\times$ faster than the pure
Python code replaced.

\myKey{Aetherling:}
In summer 2018, as a research assistant at Stanford, I contributed to
the Aetherling DSL (``Type-Directed Scheduling of Streaming
Accelerators'', PLDI 2020). This Haskell-embedded DSL compiles an
input DAG for an image processing pipeline into an FPGA design, which
is auto-parallelized (to fit the chip) using a type system that
encodes interface and scheduling information for each DAG vertex.

In this early phase of the project, I made a couple of practical
contributions (a functional simulator and some test cases for
generated hardware designs), but most of my work was in prototyping
language design and the type system.  Most notably, I identified that
the type system rules interacted poorly with the line buffer's 2D
interface, and would have made it impossible (except special cases) to
express a valid line buffer with a parallelization factor other than
one. I proposed a new way of representing line buffer state that fixed
this limitation, and assisted David Durst in seeing this new line
buffer realized in a hardware design language.

I also prototyped type systems for encoding scheduling information
(latency, lane count, underutilization) and algorithms for automatic
delay-matching.

\filbreak

\myKey{NVIDIA:}
My recent work has been on developing novel GPU algorithms for 2D
computational geometry, both as internal research in its own right,
and in support of the (separate) cuLitho computational lithography
project. The research software was presented at GTC 2023 as
\webText{https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51140/}{session
s55140}, ``How to Accelerate 2D Shape Processing for Manufacturing and
Planning''. I primarily work with Mark Kilgard on this topic, although
the majority of the algorithms, and all of the research code, were
developed by me.

As a concrete example of a motivating problem, consider cubic BÃ©zier
fitting. A typical algorithm for this generates a candidate cubic
segment using a reduction over the input point sequence, and accepts
only if the max deviation is low enough. If not, a heuristic chooses
where to split the input into two sub-sequences, and we recursively
fit to \textit{both} subdivided sequences (thus the recursion is
non-trivial, and the size of the reduction is data-dependent).

To port this to the GPU, I had to
\begin{enumerate}
  \item Assign one thread to each input point, and rewrite all loops.

  \item Transform the recursion to tail recursion (each point/thread
  is now involved in just one branch of the subdivide recursion).

  \item Efficiently parallelize these unpredictable-length reductions (which pay
  no heed to warp or block boundaries).
\end{enumerate}

Although I've taken pains to keep the codebase clear and documented,
the fact remains the actual algorithm can only be seen through a
distorting lens, dimly, past all the atomics, interleaved
synchronization, and comments explaining these complicated
transformations.  Wouldn't it be nice if there were clearer
abstractions for expressing these manually-applied ``rewrite rules''?

% Mad-lib Statement of Purpose generation
% Not great but I make the strategic call to use almost all of my 1000
% words to describe my distinctive background ... if you're reading this
% hopefully you're not me in 6 months regretting this choice.

\myKey{Future:}
But of course, setting up a PL ambitious enough to abstract away messy
problems like the above is probably somewhere between a ``postdoc''
and ``never'' project, and that's fine. My immediate interests are to
contribute to, and gain experience from, more realistic ways to push
the state of the art for parallel algorithms.
\ifdefined\STANFORD
I envision future work in the spirit of projects like Madrona, the
Mosaic compiler, or the Agile Hardware Project.
\fi
\ifdefined\UW
UW in particular has active research in DSLs targetting
custom/low-power hardware (e.g. Exo and Chlorophyll), which suits my
interests well.
\fi
In the long term (maybe late PhD; maybe postdoc), with the benefit of
experience gained, I hope to be able to form a more informed opinion
on what ways
\ifdefined\UW
(and to what extent)
\fi
such work can be extended to form a viable framework for
parallelizing general-purpose algorithms.

\end{document}

% Local Variables:
% mode: tex
% End:
